{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Climate Networks of Indian Monsoon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will explore patterns of rainfall in India during the monsoon (June-July-August-September; JJAS) season.  \n",
    "We will proceed similarly as in the tutorial:  \n",
    "1. Load data and preprocess \n",
    "2. Pairwise-Intercomparison between all time series\n",
    "3. Generate Adjacency\n",
    "4. Generate Network and analyze its communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plot_utils as put\n",
    "import scipy.stats as st\n",
    "import networkx as nx  # For network analysis\n",
    "import networkit as nk  # For community detection\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Familiar with the data\n",
    "The data is loaded using the package xarray.  \n",
    "\n",
    "**Exercise :** The data is provided as daily data. However, precipititation data is very stochastic.  \n",
    "We therefore analyze weekly data, to better average out daily variations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to an xarray dataset\n",
    "ds = xr.open_dataset('./data/mswep_pr_1_india_jjas_ds.nc')\n",
    "# Resample the dataset to weekly values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Get familiar with the annual means and the Quantiles:  \n",
    "Plot the mean precipitation over the Indian JJAS monsoon season using cartopy as well as \n",
    "the 0.9 quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(put)\n",
    "var_name = 'pr'\n",
    "mean_pr = None # Compute the mean and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute anomaly time series\n",
    "**Exercise**  Plot the time series of the average precipitation over India. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average JJAS rainfal\n",
    "# Are here any problems?\n",
    "# Use ds[var_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**  Compute next the day of year anomalies. Do you think we have to detrend the data? Why/Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly time series \n",
    "# Group each time point by its corresponding day of the year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the trends and plot for particular cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen from the linear fit, that the linear decrease/increase is very little.  \n",
    "We can therefore conclude that there is no clear trend in the precipitation data over the last 40 years.  \n",
    "Therefore, we do not need to detrend the data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the data is prepared to be used properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ds['anomalies']  # Use the anomaly data to compute the pairwise correlations\n",
    "print('Dataset shape: ', da.shape)\n",
    "dim_time, dim_lat, dim_lon = da.shape\n",
    "# Bring all into a form of an array of time series\n",
    "data = []\n",
    "data = []\n",
    "for idx, t in enumerate(da.time):\n",
    "        buff = da.sel(time=t.data).data.flatten()  # flatten each time step\n",
    "        buff[np.isnan(buff)] = 0.0  # set missing data to climatology\n",
    "        data.append(buff)\n",
    "data = np.array(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute all pair-wise correlations using the Spearman's rank order correlation.  \n",
    "\n",
    "*Hint: Pay attention to exclude all non-significant correlation values! Take a confidence level of 99.9%.*\n",
    "\n",
    "**Exercise:** Compute the minimum value of the correaltion that is still accounted as a significant.  \n",
    "What do you think? Is this a good threshold value? Compute the adjacency matrix for different thresholds.  \n",
    "What do you think is a good density for the adjecency matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Flattend Dataset shape: ', data.shape)\n",
    "corr, pvalue =  None # .... \n",
    "print('Shape of correlation Matrix: ', corr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all correlations are statistically significant.\n",
    "Let's first exclude non-significant correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.999\n",
    "# Exclude non-significant values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally compute the adjacency matrix of the network. \n",
    "Think about how you would choose correlation threshold.  \n",
    "What might be a problem of too high/low thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = None # Set a treshold, can be 0\n",
    "# compute adjacency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ideal density of the network should be around 5-10%. Setting the threshold to different \n",
    "values will change the density accordingly.  \n",
    "If we finally have the adjacency, we can create an networkx object based on the adjacency.  \n",
    "Create a networkx object of the adjacency matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the network is transformed to a networkx object. For this the adjecency has to be a numpy array of shape ($lon\\times lat, lon\\times lat$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use networkx for better using dealing with the adjacency matrix\n",
    "import networkx as nx\n",
    "cnx = nx.DiGraph(adjacency)\n",
    "\n",
    "# Set the longitude and latitude as node attributes\n",
    "lons = ds.lon\n",
    "lats = ds.lat\n",
    "lon_mesh, lat_mesh = np.meshgrid(lons, lats)  # This gives us a list of longitudes and latitudes per node\n",
    "nx.set_node_attributes(cnx, {node: lon_mesh.flatten()[node] for node in cnx.nodes()}, 'lon')\n",
    "nx.set_node_attributes(cnx, {node: lat_mesh.flatten()[node] for node in cnx.nodes()}, 'lat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make first steps to analyze the network.   \n",
    "**Exercise:** Compute the node degree of a node $i$ of the network is computed using the Adjacency matrix $A$:  \n",
    "$$ k_i = \\sum_i A_{ij} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the node degree and plot it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute the Betweenness Centrality\n",
    "$$\n",
    "BC_v(v_i) = \\sum_{s,t}^N \\frac{\\sigma(v_s, v_t|v_i)}{\\sigma(v_s, v_t)} \\; ,  \n",
    "$$\n",
    "where $\\sigma (v_s,v_t)$ denotes the number of shortest paths between nodes $v_s$ and $v_t$ and $\\sigma(v_s,v_t | v_i) \\leq \\sigma(v_s,v_t)$ the number of all shortest paths that include node $v_i$.  \n",
    "\n",
    "*Hint: Look at the [documentation](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.betweenness_centrality.html#networkx.algorithms.centrality.betweenness_centrality)*\n",
    "\n",
    "You can also try out other network measure.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Betweenness centrality and plot it\n",
    "# Use the Betweenness centrality function from networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(put)\n",
    "\n",
    "# Plot BC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**  Compare your results with [Stolbova et al., 2014](https://npg.copernicus.org/articles/21/901/2014/). Do you find similarities/differences?  \n",
    "Note, that current literature uses for precipitation analysis often another similarity measure than Spearman's correlation!  \n",
    "Can you provide an explanation for the concentration of links to the western coast of India?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize single edges of the network\n",
    "**Exercise:**  To better analyze single parts of the network we want to extract the links from multiple specific regions.\n",
    "For the precipitation network, do you spot any particular differences to the global 2m-air temperature networks?  \n",
    "\n",
    "*Hint: As an example try different locations at the coast, at mountain areas, at high/low latitudes etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this by 3 consecutive steps:\n",
    "1. Find out the source node ids of the region of which you want to analyze the outgoing links\n",
    "2. Uncover all the edges to this region, using the adjacency or the networkx package (called target nodes)\n",
    "3. Find out the spatial locations of the target nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the nodes of the source region\n",
    "lat_range = [20, 25]\n",
    "lon_range = [75,78]\n",
    "# Why is this masked needed?\n",
    "mean_ds = ds[var_name].mean(dim='time')\n",
    "mask = (\n",
    "        (mean_ds['lat'] >= min(lat_range))\n",
    "        & (mean_ds['lat'] <= max(lat_range))\n",
    "        & (mean_ds['lon'] >= min(lon_range))\n",
    "        & (mean_ds['lon'] <= max(lon_range))\n",
    "        )\n",
    "source_map =  # Fill this out\n",
    "\n",
    "# Plot source Ids here for control\n",
    "\n",
    "# Get Ids of locations\n",
    "source_ids = np.where(source_map.data.flatten()==1)[0]  # flatten data and get position in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find target Ids in the network\n",
    "edge_list = []\n",
    "for sid in source_ids:\n",
    "        edge_list.append(list(cnx.edges(sid)))\n",
    "\n",
    "edge_list = np.concatenate(edge_list, axis=0)  # transform to 2d np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Edges here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection in climate Networks\n",
    "Now we want to see what is the overall structure of the network.  \n",
    "Therefore we want to identify communities in the network. There are many algorithms to detect communities in graphs.  \n",
    "\n",
    "**Exercise:** Use the standard [Louvain algorithm](https://en.wikipedia.org/wiki/Louvain_method) from the [NetworKit](https://networkit.github.io/dev-docs/notebooks/Community.html) package to identify communities in the climate network. \n",
    "\n",
    "*Hint: Run this algorithm multiple times. Do you notice anything? Where do the differences come from? For this read the documentation of the implementations.*  \n",
    "\n",
    "What might be a solution for this problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nk algorithm needs the nx network to be transformed as a nk object\n",
    "cnk = nk.nxadapter.nx2nk(cnx.to_undirected())\n",
    "# Use the Parallel Louvain Method (PLM) of NetworkIt\n",
    "nkCommunities = None # Fill this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Communities here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**  Can you explain the different communities? Try to compare the communities with different orographic zones and connect this then back to overall climate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of climate data\n",
    "\n",
    "**Exercise :** Compute the clusters of based on complete linkage clustering of the Spearman's Correlation Matrix!  \n",
    "You might follow the method from [Rheinwalt et al. 2015](https://link.springer.com/chapter/10.1007/978-3-319-17220-0_3), moreover our results can be compared to [Malik et al., 2010]( www.nonlin-processes-geophys.net/17/371/2010/) .  \n",
    "You can use the functions below or try out another clustering Algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(corr, pvalue, confidence=0.999, threshold=None):\n",
    "    \"\"\"Get correlation and distance threshold for a given confidence level.\n",
    "\n",
    "    Note: only positive correlations are considered here\n",
    "\n",
    "    Return:\n",
    "    -----\n",
    "    threshold: float\n",
    "        Threshold where the clustering is stopped\n",
    "    distance: np.ndarray (n, n)\n",
    "        Distance matrix\n",
    "    corr_pos: np.ndarray (n, n)\n",
    "        Correlation matrix with only positive correlations\n",
    "    \"\"\"\n",
    "    # get only absolute correlations\n",
    "    corr_pos = np.abs(corr)\n",
    "\n",
    "    # get distance matrix\n",
    "    distance = np.arccos(corr_pos)\n",
    "\n",
    "    # consider only correlations with corresponding pvalues smaller than (1-confidence)\n",
    "    mask_confidence = np.where(pvalue <= (\n",
    "        1 - confidence), 1, 0)  # p-value test\n",
    "    corr_pos = np.where(mask_confidence == 1, corr_pos, np.nan)\n",
    "\n",
    "    # get threshold\n",
    "    if threshold is None:\n",
    "        idx_min = np.unravel_index(\n",
    "                np.nanargmin(corr_pos.data), np.shape(corr_pos.data)\n",
    "            )\n",
    "    else:\n",
    "        mask_corr = np.where(corr_pos >= threshold, \n",
    "                             corr_pos, np.nan)\n",
    "        idx_min = np.unravel_index(\n",
    "                np.nanargmin(mask_corr.data), np.shape(corr_pos.data)\n",
    "            )\n",
    "    threshold_corr = corr_pos[idx_min]\n",
    "    threshold_dist = distance[idx_min]\n",
    "    \n",
    "    print(f\"p-value {pvalue[idx_min]}, \\n\",\n",
    "          f\"correlation {threshold_corr} \\n\",\n",
    "          f\"Min distance threshold {threshold_dist}\")\n",
    "\n",
    "    return distance, threshold_dist\n",
    "\n",
    "def complete_linkage_cluster(distance, threshold=None, linkage=\"complete\", n_clusters=None):\n",
    "        \"\"\"Complete linkage clustering.\n",
    "        Return:\n",
    "        -------\n",
    "        labels: list (n)\n",
    "            Cluster label of each datapoint\n",
    "        model: sklearn.cluster.AgglomerativeClustering\n",
    "            Complete linkage clustering model\n",
    "        \"\"\"\n",
    "        # Use Scipy Agglomerative Clustering for distances clustering!\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        if n_clusters is not None:\n",
    "            # Exactly one of n_clusters and distance_threshold has to be set,\n",
    "            # and the other needs to be None. Here we set n_clusters if given!\n",
    "            threshold = None\n",
    "        \n",
    "        # create hierarchical cluster\n",
    "        model = AgglomerativeClustering(\n",
    "            distance_threshold=threshold, \n",
    "            n_clusters=n_clusters, \n",
    "            compute_full_tree=True,\n",
    "            affinity='precomputed', \n",
    "            connectivity=None, \n",
    "            linkage=linkage\n",
    "        )\n",
    "        labels = model.fit_predict(distance)\n",
    "        print(\n",
    "            f\"Found {np.max(labels)+1} clusters for the given threshold {threshold}.\")\n",
    "        return labels, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Clusters here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Climate Networks to PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Climate networks represent a non-linear transformations of the data in order to reduce the dimensionality of the data. PCA is a linear transformation used as well for dimensionality reduction. We can compare the Principial Components to the Network measures to climate network.\n",
    "\n",
    "**Exercise :**  Apply a PCA on the precipitation anomaly data, visualize the EOF map of the first two components. What do you see by comparing them to node degree plots of the climate network? Do you have an explanation for this similarity?\n",
    "\n",
    "*Hint: You might have a look at [Donges et al., 2015](https://link.springer.com/article/10.1007/s00382-015-2479-3)!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot EOF maps\n",
    "i = 0\n",
    "eof_map = put.create_map_for_da(da=ds[var_name],\n",
    "                                data=# File in here data,\n",
    "                                name=f'EOF{i}')\n",
    "\n",
    "\n",
    "im['ax'].set_title(f\"EOF {i+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Node Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d383b1a48ef01212f0b28a42cb19233e79018349d487f3ef9eded2e8f8db7866"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('monsoon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

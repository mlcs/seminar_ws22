{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Climate Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we explain the basic conecpt of climate networks. \n",
    "This is done in for consecutive steps:  \n",
    "1. Load data and preprocess \n",
    "2. Pairwise-Intercomparison between all time series\n",
    "3. Generate adjacency matrix of the pairwise-intercomparison\n",
    "4. Generate Network and analyze it   \n",
    "\n",
    "Below the scetch visualizes the procedure how to construct the climate network\n",
    "\n",
    "  \n",
    "<img src=\"img/climate_network.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plot_utils as put\n",
    "import scipy.stats as st\n",
    "import networkx as nx  # For network analysis\n",
    "import networkit as nk  # For community detection\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "### Getting familiar with the data\n",
    "\n",
    "Many atmospheric phenomena are driven by the ocean-atmosphere interactions, since both are together responsible for the transfer of heat on our planet. Surface air temperatures (SST) fields can help us predict and understand many of this coupled atmosphere-ocean processes. For example, anomalies in the Pacific SST can lead to large climatic changes world wide. \n",
    "\n",
    "The monthly 2m-air temperature data from 1979-2021 are taken from [Copernicus ERA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels-monthly-means?tab=overview). The data are regridded on a $2.5^\\circ \\times 2.5^\\circ$ resolution.\n",
    "\n",
    "We use xarray to read the .nc files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to an xarray dataset\n",
    "ds = xr.open_dataset('./data/era5_t2m_2.5_monmean_ds.nc')\n",
    "# Just executing the dataset will give you a first overview over the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a first impression of the data. Are they meaningful?\n",
    "var_name = 't2m'  # This is the name of the climate variable = 2 meter air temperature\n",
    "ds[var_name].mean(dim='time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For nicer plots we can use cartopy. This however also requires many more lines of codes which you can find in the plot_utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(put)\n",
    "mean_t2m = ds[var_name].mean(dim='time')\n",
    "im = put.plot_map(mean_t2m, label='Global mean temperature [K]', \n",
    "                  projection='EqualEarth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "So far we have the raw data. Before we can however start with analysing the data more in detail we have to account for a few pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detrend data and create anomaly time series\n",
    "Plot the average global mean temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average yearly temperature\n",
    "# What might be the problem?\n",
    "ds[var_name].mean(dim='lon').mean('lat').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data we want to avoid correlations that arise due **to the seasonal cycle**. Therefore we compute anomaly time series in each cell:\n",
    "* The anomaly of a variable is its variation relative to the **climatological mean**.  \n",
    "* Climatological means can be based on the day of the year, month, season, year etc ...  \n",
    "* The mean is the long-term average of the same variable. We use it as a baseline value.   \n",
    "* The anomaly values are therefore the **data - climatology** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly time series \n",
    "# Group each time point by its corresponding day of the year\n",
    "group = 'dayofyear'\n",
    "climatology = (\n",
    "            ds[var_name].groupby(f\"time.{group}\").mean(dim=\"time\")\n",
    "        )\n",
    "# Compute the anomalies\n",
    "anomalies = ds[var_name].groupby(f\"time.{group}\") - climatology\n",
    "anomalies.mean(dim='lon').mean('lat').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot we see **a constant increase** over the last 40 years due to global warming! \n",
    "We therefore need to detrend the data.  \n",
    "We do this by computing in each cell the trend and substract it then from the original dataset.  \n",
    "The resulting dataset is then plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the fit coefficients\n",
    "p = ds[var_name].polyfit(dim='time', deg=1)\n",
    "# Apply the fit\n",
    "fit = xr.polyval(ds['time'], p.polyfit_coefficients)\n",
    "# The fit gives us the linear increase in each cell\n",
    "da_detrend = ds[var_name] - fit  # Substract the linear increase from the actual data\n",
    "da_detrend.mean(dim='lon').mean('lat').plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute anomaly time series \n",
    "group = 'dayofyear'\n",
    "climatology = (\n",
    "            da_detrend.groupby(f\"time.{group}\").mean(dim=\"time\")\n",
    "        )\n",
    "anomalies = da_detrend.groupby(f\"time.{group}\") - climatology\n",
    "anomalies.mean(dim='lon').mean('lat').plot()\n",
    "ds['anomalies'] = anomalies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute the adjacency matrix\n",
    "Now we are ready to start analyzing the data. We use the scipy library which is very similar to numpy. In order to use our SST anomaly data as inputs to neural networks (NN) we have however still have to preprocess the data. The following preprocessing steps are usually required: \n",
    "\n",
    "1. **Remove missing data**: Some datasets contain missing data. We set this data to zero (=the climatology)\n",
    "\n",
    "2. **Reshape**: So far the data is in the shape (time, lon, lat). In order to being ready to be used it has to be a 2-dimensional array containing all time series as rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ds['anomalies']  # Use the anomaly data to compute the pairwise correlations\n",
    "print('Dataset shape: ', da.shape)\n",
    "dim_time, dim_lat, dim_lon = da.shape\n",
    "# Bring all into a form of an array of time series\n",
    "data = []\n",
    "data = []\n",
    "for idx, t in enumerate(da.time):\n",
    "        buff = da.sel(time=t.data).data.flatten()  # flatten each time step\n",
    "        buff[np.isnan(buff)] = 0.0  # set missing data to climatology\n",
    "        data.append(buff)\n",
    "data = np.array(data)\n",
    "print('Flattend Dataset shape: ', data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the Spearman's correlation function $\\rho$.  \n",
    "\n",
    "<img src=\"img/Spearman.png\" width=\"300\"> [Ref](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient#/media/File:Spearman_fig1.svg)\n",
    "\n",
    "We do it to compute the similarity between all time series $t_i$, $t_j$: \n",
    "$$\n",
    "C_{ij} = \\rho(t_i, t_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a correlation matrix $C$ where $C_{ij} = \\rho(t_i, t_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, pvalue =  st.spearmanr(\n",
    "        data, axis=0, nan_policy='propagate')\n",
    "print('Shape of correlation Matrix: ', corr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all correlations are statistically significant.\n",
    "Let's first exclude non-significant correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.999\n",
    "mask_confidence = np.where(pvalue <= (1 - confidence), 1, 0)  # p-value test\n",
    "corr = np.where(mask_confidence==1, corr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get minimum value that is still accounted as a correlation\n",
    "# Do you think this is fine?\n",
    "np.nanmin(np.where(np.abs(corr) != 0, np.abs(corr), np.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally compute the adjacency matrix of the network.  \n",
    "This means to exclude values that are smaller than $\\rho_{min} = 0.4$  and set them to 0:  \n",
    "$$\n",
    "\\mathbf{A}_{ij} = \n",
    "    \\begin{cases}\n",
    "    1, & |C_{ij}| > \\rho_{min} \\\\\n",
    "    0, & \\textrm{otherwise}\n",
    "    \\end{cases},\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4  # absolute threshold for correlations\n",
    "mask_correlation = np.where(np.abs(corr) >= threshold, 1, 0)\n",
    "\n",
    "# A link in the adjacency is placed for all significant values above the threshold\n",
    "adjacency = mask_confidence * mask_correlation\n",
    "\n",
    "# Obtain density of adjacency matrix.\n",
    "density = (\n",
    "    np.count_nonzero(adjacency.flatten()) / adjacency.shape[0]**2\n",
    ")\n",
    "print(\"density of adjacency: \", density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the Adjacency $A$ of the network. \n",
    "Can you try to explain the patterns you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(X=adjacency, cmap='Greys',  interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analzye the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To more easily analyze the network we use the [networkx package](https://networkx.org/), using that we have the adjacency.\n",
    "Networkx provides powerful functions to further analyse the graph's topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use networkx for better using dealing with the adjacency matrix\n",
    "import networkx as nx\n",
    "cnx = nx.DiGraph(adjacency)\n",
    "\n",
    "# Set the longitude and latitude as node attributes\n",
    "lons = ds.lon\n",
    "lats = ds.lat\n",
    "lon_mesh, lat_mesh = np.meshgrid(lons, lats)  # This gives us a list of longitudes and latitudes per node\n",
    "nx.set_node_attributes(cnx, {node: lon_mesh.flatten()[node] for node in cnx.nodes()}, 'lon')\n",
    "nx.set_node_attributes(cnx, {node: lat_mesh.flatten()[node] for node in cnx.nodes()}, 'lat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first very commonly used step is to look at the node degree. It can give a you a first very informative impression of the network.   \n",
    "The node degree of a node $i$ of the network is computed using the Adjacency matrix $A$:    \n",
    "$$ k_i = \\sum_i A_{ij} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the node degree and plot it\n",
    "reload(put)\n",
    "degrees = np.array(cnx.degree())[:,1] # This returns the list of degrees\n",
    "# Plot degrees\n",
    "degree_arr = np.array(cnx.degree())[:,1].reshape(dim_lat, dim_lon)\n",
    "degree_map = put.create_map_for_da(da=ds[var_name], data=degree_arr, name='degree')\n",
    "im = put.plot_map(degree_map, \n",
    "                  label='Node Degree', \n",
    "                  projection='EqualEarth',\n",
    "                  vmin=0,\n",
    "                  vmax=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Oscillation Modes\n",
    "\n",
    "The largest interanual variability of the climate is characterized by SST anomalies in the tropical Pacific, called the **El Nino Southern Oscillation (ENSO)**. ENSO has a normal, warm phase called **El Nino** and a cold phase called **La Nina** with different impacts on global climate. \n",
    "\n",
    "The **Indian Ocean Dipole (IOD)**, also known as the Indian Nino, is an irregular oscillation of sea surface temperatures in which the western Indian Ocean becomes alternately warmer (positive phase) and then colder (negative phase) than the eastern part of the ocean. It is known to be strongly correlated to the ENSO. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize single edges of the network\n",
    "To better analyze single parts of the network we want to extract the links from a specific region.  \n",
    "As an example we now look at the red spot in the Indian Occean Dipole and try to visualize where the links of this region go to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the nodes of the source region\n",
    "lat_range = [-10, -15]\n",
    "lon_range = [60,65]\n",
    "# Why is this masked needed?\n",
    "mask = (\n",
    "        (degree_map['lat'] >= min(lat_range))\n",
    "        & (degree_map['lat'] <= max(lat_range))\n",
    "        & (degree_map['lon'] >= min(lon_range))\n",
    "        & (degree_map['lon'] <= max(lon_range))\n",
    "        )\n",
    "source_map = xr.where(mask, 1, np.nan)\n",
    "im = put.plot_map(source_map, \n",
    "                  bar=False, \n",
    "                  projection='EqualEarth',\n",
    "                  central_longitude=60)\n",
    "\n",
    "# Get Ids of locations\n",
    "source_ids = np.where(source_map.data.flatten()==1)[0]  # flatten data and get position in array\n",
    "\n",
    "# Find target Ids in the network\n",
    "edge_list = []\n",
    "for sid in source_ids:\n",
    "        edge_list.append(list(cnx.edges(sid)))\n",
    "\n",
    "edge_list = np.concatenate(edge_list, axis=0)  # transform to 2d np array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotting of edges is rather complicated. Therefore we use again an own defined plotting \n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(put)\n",
    "# Plot edges from this source region\n",
    "im = put.plot_map(source_map, \n",
    "                  bar=False, \n",
    "                  projection='EqualEarth',\n",
    "                  central_longitude=60)\n",
    "put.plot_edges(cnx=cnx,\n",
    "               edges=edge_list[::10],\n",
    "               ax=im['ax'])  # Plot every 10th\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Community detection in climate networks\n",
    "<img src=\"img/Network_Community_Structure.png\" width=\"300\"> [Ref](https://en.wikipedia.org/wiki/Community_structure)\n",
    "\n",
    "Now we want to see what is the overall structure of the network.  \n",
    "Therefore we want to identify communities in the network. There are many algorithms to detect communities in graphs.  \n",
    "Here, we use the standard [Louvain algorithm](https://en.wikipedia.org/wiki/Louvain_method) from the [NetworKit](https://networkit.github.io/dev-docs/notebooks/Community.html) package.  \n",
    "It basically optimizes local density of edges to cluster a given graph into communities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Community detection works:  \n",
    "\n",
    "<img src=\"img/Network_community_graph.png\" width=\"400\"> [Ref](https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0232-2)\n",
    "<img src=\"img/communities_adjacency.png\" width=\"400\"> [Ref](https://appliednetsci.springeropen.com/articles/10.1007/s41109-019-0232-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nk algorithm needs the nx network to be transformed as a nk object\n",
    "cnk = nk.nxadapter.nx2nk(cnx.to_undirected())\n",
    "# Use the Parallel Louvain Method (PLM) of NetworkIt\n",
    "nkCommunities = nk.community.detectCommunities(\n",
    "            cnk, algo=nk.community.PLM(cnk, True)\n",
    "        )\n",
    "CommunityIds = np.array(nkCommunities.getVector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "community_map = put.create_map_for_da(da=ds[var_name], data=CommunityIds.reshape(dim_lat, dim_lon), name='Communities')\n",
    "im = put.plot_map(community_map, bar=True, \n",
    "                  cmap='rainbow',\n",
    "                  projection='EqualEarth',\n",
    "                  central_longitude=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering of Climate Data Time Series\n",
    "Compute the clusters of based on complete linkage clustering of the Spearman's Correlation Matrix!  \n",
    "In order to group time series by similarity we use the standard fast greedy hierarchical agglomerative complete linkage clustering].  \n",
    "This clustering is performed  in a metric space with dissimilarities between time series as distances.  \n",
    "\n",
    "\n",
    "We follow the method from [Rheinwalt et al. 2015](https://link.springer.com/chapter/10.1007/978-3-319-17220-0_3).\n",
    "The distance function is here the cosine of the correlation:  \n",
    "$$ \\rho_{X,Y} = \\cos (\\theta_{X,Y}) $$\n",
    "where the angle can have values $\\theta_{X,Y} = 0,\\dots, 2\\pi$.  \n",
    "This ensures that as more similar two time series $X,Y$ are the smaller their distance is!\n",
    "We use complete linkage implementation from [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html).\n",
    "\n",
    "Having computed the distances we can proceed computing the dendrogram:  \n",
    "\n",
    "<img src=\"img/example_dendrogram.png\" width=\"600\"> [Ref](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(corr, pvalue, confidence=0.999, threshold=None):\n",
    "    \"\"\"Get correlation and distance threshold for a given confidence level.\n",
    "\n",
    "    Note: only positive correlations are considered here\n",
    "\n",
    "    Return:\n",
    "    -----\n",
    "    threshold: float\n",
    "        Threshold where the clustering is stopped\n",
    "    distance: np.ndarray (n, n)\n",
    "        Distance matrix\n",
    "    corr_pos: np.ndarray (n, n)\n",
    "        Correlation matrix with only positive correlations\n",
    "    \"\"\"\n",
    "    # get only absolute correlations\n",
    "    corr_pos = np.abs(corr)\n",
    "\n",
    "    # get distance matrix\n",
    "    distance = np.arccos(corr_pos)\n",
    "\n",
    "    # consider only correlations with corresponding pvalues smaller than (1-confidence)\n",
    "    mask_confidence = np.where(pvalue <= (\n",
    "        1 - confidence), 1, 0)  # p-value test\n",
    "    corr_pos = np.where(mask_confidence == 1, corr_pos, np.nan)\n",
    "\n",
    "    # get threshold\n",
    "    if threshold is None:\n",
    "        idx_min = np.unravel_index(\n",
    "                np.nanargmin(corr_pos.data), np.shape(corr_pos.data)\n",
    "            )\n",
    "    else:\n",
    "        mask_corr = np.where(corr_pos >= threshold, \n",
    "                             corr_pos, np.nan)\n",
    "        idx_min = np.unravel_index(\n",
    "                np.nanargmin(mask_corr.data), np.shape(corr_pos.data)\n",
    "            )\n",
    "    threshold_corr = corr_pos[idx_min]\n",
    "    threshold_dist = distance[idx_min]\n",
    "    \n",
    "    print(f\"p-value {pvalue[idx_min]}, \\n\",\n",
    "          f\"correlation {threshold_corr} \\n\",\n",
    "          f\"Min distance threshold {threshold_dist}\")\n",
    "\n",
    "    return distance, threshold_dist\n",
    "\n",
    "def complete_linkage_cluster(distance, threshold=None, linkage=\"complete\", n_clusters=None):\n",
    "        \"\"\"Complete linkage clustering.\n",
    "        Return:\n",
    "        -------\n",
    "        labels: list (n)\n",
    "            Cluster label of each datapoint\n",
    "        model: sklearn.cluster.AgglomerativeClustering\n",
    "            Complete linkage clustering model\n",
    "        \"\"\"\n",
    "        # Use Scipy Agglomerative Clustering for distances clustering!\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        compute_full_tree=True,\n",
    "        if n_clusters is not None:\n",
    "            # Exactly one of n_clusters and distance_threshold has to be set,\n",
    "            # and the other needs to be None. Here we set n_clusters if given!\n",
    "            threshold = None\n",
    "            compute_full_tree = False\n",
    "        \n",
    "        # create hierarchical cluster\n",
    "        model = AgglomerativeClustering(\n",
    "            distance_threshold=threshold, \n",
    "            n_clusters=n_clusters, \n",
    "            compute_full_tree=compute_full_tree,\n",
    "            compute_distances=True,\n",
    "            affinity='precom    puted', \n",
    "            connectivity=None, \n",
    "            linkage=linkage\n",
    "        )\n",
    "        labels = model.fit_predict(distance)\n",
    "        print(\n",
    "            f\"Found {np.max(labels)+1} clusters for the given threshold {threshold}.\")\n",
    "        return labels, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ds['anomalies']  # Use the anomaly data to compute the pairwise correlations\n",
    "print('Dataset shape: ', da.shape)\n",
    "dim_time, dim_lat, dim_lon = da.shape\n",
    "# Bring all into a form of an array of time series\n",
    "data = []\n",
    "data = []\n",
    "for idx, t in enumerate(da.time):\n",
    "        buff = da.sel(time=t.data).data.flatten()  # flatten each time step\n",
    "        buff[np.isnan(buff)] = 0.0  # set missing data to climatology\n",
    "        data.append(buff)\n",
    "data = np.array(data)\n",
    "print('Flattend Dataset shape: ', data.shape)\n",
    "corr, pvalue =  st.spearmanr(\n",
    "        data, axis=0, nan_policy='propagate')\n",
    "print('Shape of correlation Matrix: ', corr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance, threshold_dist = get_distance(corr=corr, pvalue=pvalue, threshold=0.12)\n",
    "clusterIds, model = complete_linkage_cluster(distance=distance, \n",
    "                                             threshold=threshold_dist,\n",
    "                                             n_clusters=None)  # Or use n=9 like the outcome of the community detection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(put)\n",
    "cluster_map = put.create_map_for_da(da=ds[var_name], data=clusterIds.reshape(dim_lat, dim_lon), name='Cluster')\n",
    "im = put.plot_map(cluster_map, bar=True, \n",
    "                  cmap='rainbow',\n",
    "                  projection='EqualEarth',\n",
    "                  central_longitude=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(put)\n",
    "_ = put.fancy_dendrogram(model=model, \n",
    "                     truncate_mode=\"lastp\",\n",
    "                     p=9\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d383b1a48ef01212f0b28a42cb19233e79018349d487f3ef9eded2e8f8db7866"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('monsoon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
